# Skip Gram Model

## Key Concepts Learned:
- **Text Preprocessing**  
  The process of cleaning and preparing text data for analysis, including steps like tokenization, stopword removal, and stemming.

- **Word Embedding**  
  A technique to represent words as dense vectors in a continuous vector space, allowing words with similar meanings to have similar representations.

# Transformer Model

## Key Concepts Learned:
- **Full Architecture**  
  Understanding the core structure of the transformer model, which includes an encoder to process input data and a decoder to generate the output, utilizing self-attention mechanisms for better context understanding.

- **Tokenizing**  
  The process of splitting text into smaller units (tokens) like words or subwords, making it easier for the transformer model to process and understand.

- **Positional Encoding**  
  A technique to inject information about the position of tokens in the sequence, enabling the transformer to maintain the order of words, which is crucial since the model lacks inherent sequential processing.

- **Multi-Head Attention**  
  An extension of self-attention, where multiple attention mechanisms (or "heads") work in parallel to capture different aspects of the data and improve performance.

- **BERT (Bidirectional Encoder Representations from Transformers)**  
  A pre-trained transformer model that processes text bidirectionally, capturing context from both the left and right of a token, making it highly effective for a wide range of NLP tasks.

- **LoRA (Low-Rank Adaptation)**  
  A technique that reduces the computational cost and memory requirements of training large models by applying low-rank updates to the pre-trained transformer weights, enabling more efficient fine-tuning.
