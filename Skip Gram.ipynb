{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "<font>\n",
        "<div dir=ltr align=center>\n",
        "\n",
        "<font color=0F5298 size=7>\n",
        "    Machine learning <br>\n",
        "<font color=2565AE size=5>\n",
        "    Computer Engineering Department <br>\n",
        "    Fall 2024<br>\n",
        "<font color=3C99D size=5>\n",
        "    NLP - Skip-Gram <br>\n",
        "<font color=0CBCDF size=4>\n",
        "  \n",
        "</div>\n",
        "\n",
        "____"
      ],
      "metadata": {
        "id": "0QOmoitc9qC4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-10T13:12:41.263643Z",
          "iopub.status.busy": "2020-10-10T13:12:41.262979Z",
          "iopub.status.idle": "2020-10-10T13:12:49.471771Z",
          "shell.execute_reply": "2020-10-10T13:12:49.471205Z"
        },
        "id": "hoV5vSSSbIp0",
        "outputId": "4915ab2d-4a0a-4dd1-8d0b-606f34be02d4",
        "papermill": {
          "duration": 8.238801,
          "end_time": "2020-10-10T13:12:49.471884",
          "exception": false,
          "start_time": "2020-10-10T13:12:41.233083",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import io\n",
        "import math\n",
        "import gzip\n",
        "import nltk\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import gensim.downloader as api\n",
        "import tensorflow_datasets as tfds\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import skipgrams\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pwuegqx-JWf",
        "papermill": {
          "duration": 0.023154,
          "end_time": "2020-10-10T13:12:49.521385",
          "exception": false,
          "start_time": "2020-10-10T13:12:49.498231",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### Downloading Dataset\n",
        "We're going to use text8 dataset. Text8 is first 100,000,000 bytes of plain text from Wikipedia. It's mainly used for testing purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-10T13:12:49.580623Z",
          "iopub.status.busy": "2020-10-10T13:12:49.579765Z",
          "iopub.status.idle": "2020-10-10T13:13:03.778660Z",
          "shell.execute_reply": "2020-10-10T13:13:03.779347Z"
        },
        "id": "XG-FjuVEFLGW",
        "papermill": {
          "duration": 14.234694,
          "end_time": "2020-10-10T13:13:03.779495",
          "exception": false,
          "start_time": "2020-10-10T13:12:49.544801",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def load_data():\n",
        "  text8_zip_file_path = api.load('text8', return_path=True)\n",
        "  with gzip.open(text8_zip_file_path, 'rb') as file:\n",
        "    file_content = file.read()\n",
        "  wiki = file_content.decode()\n",
        "  return wiki\n",
        "\n",
        "wiki = load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYBT_-Vy_af4",
        "papermill": {
          "duration": 1.449897,
          "end_time": "2020-10-10T13:13:06.611079",
          "exception": false,
          "start_time": "2020-10-10T13:13:05.161182",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### Preprocessing data\n",
        "\n",
        "**Stopwords removal** - Begin by removing stopwords from the dataset, as they provide little to no value in learning word embeddings. Ensure your preprocessing pipeline filters out commonly used words such as \"the,\" \"and,\" or \"of\" that do not contribute to meaningful semantic relationships.\n",
        "\n",
        "---\n",
        "\n",
        "**Subsampling words** - In a large corpora, most frequent words can easily occur hundreds of millions of times and such words usually don't bring much information to the table.  It is of essential importance to cut down on their frequencies to mitigate the negative impact it adds. For example, co-occurrences of \"English\" and \"Spanish\" benefit much more than co-occurrences of \"English\" and \"the\" or \"Spanish\" and \"of\". To counter the imbalance between rare and frequent words Mikolov et. al came up with the following heuristic formula for determining probability to drop a particular word:\n",
        "\n",
        "![formula.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMIAAABDCAIAAABBb00bAAAKQElEQVR42uydWUxj1R/HTymg0xkslGlnWEQFqk2HYCOdIZ1xmGHTYRIlQx2cEreEJQhEQRMaHpSIEPHBoLgkLDHBGExkMVg1RlMUWn1AgZIWSKFlp7RQW0qR7r3/xPtPc21LhS60wPk8zT33nHPPvf1yzu/8vrfTcARBAATiG2HwEUCgjCBQRhAoIwgEyggCZQSBMoJAGUEgbgiHjyC4NDU10Wg0CoUSrAHQ6XTfrw5lFGS+/PLLu3fvhoUFbVkgkUi+ywgHzZAgotfrU1NTlUolDoeDsRHES8RicXp6+nHXEJRRkBGJRAwGA+7UIFBGUEbBZnJy8mTICIbYQcNqtZLJ5K2trfBwn/bLWq02JiYGzkanFKlUmpKS4qOG5ubmLl68qFaroYxgYOQ9P/zwA5lMjo2NhTKCgZE3mEwmtVrN5/OZTKZWq4UygrORN3zxxRcVFRXfffedSqWqrKxcXl6GIfapA0EQCoUil8sfeOABrzuZnp5OS0uTSqWPPvoonI1OI+vr60Qi0RcNAQAEAkFcXByVSg367UAZHeP4WiAQ3Lx5MxS8FCijYxlfY2UEAOjt7bVaradRRgiCeHfnXjc8YbORVqtdXV29cuWKRCIRCoU+5p9CTkYmk2n737hG8RaLpby8fGFhwYv+t7e3X3rpJb1ef8plFBMTU1RU1NHR0d7e/v777wd/y+Bf3nrrLXTjkJWVVVhYmJOTk5yczOFwlEqlo05VVVVXV5fXlxgeHi4rK/PvsO12+/fff19fX48Enu3t7fPnz9vtdt+72traQkIAEIhOy8rKCASC0WhED81mM41Gy8vLQw9HR0ezsrJ8fIgvvPDCwMCAvwb8+uuvZ2VlxcfHZ2ZmHsFDHxkZyc3NRU4QAZERlUrNz8/Hlly7du2+++6zWq0Igty+fbu/v9/HS0xMTKSnp/t32M8///zRyOijjz568803T5KM/B8bKRSK+fl5dAeBolarRSJRTk4OHo/X6/UjIyO3b9/Grqqzs7OO+AlBEJvNhsZPa2tr+12FwWAolUrvoquTsdsPKfwf3o+MjAAAsrOz0cOdnZ3q6moKhdLd3Q0A+Pnnn6lU6pkzZ9CzVqv11VdfxeFwu7u7vb29AIDa2lq73f7xxx/X19f39/cvLy+7fd0dh8OxWKxvv/22trbW6ZROp7NYLPsNLzY2NuiJFpFI9MYbb0AZeeLXX38FALS2tkZERJjN5r///vv69evd3d1RUVEAALlcnpSU5Kj89ddfZ2dnK5XK9vZ2dCrq6+urr68HANy7d6+zs9NR848//pDJZBwOx1ESHx8vl8udrq5SqW7duuVBRu+9994zzzxzNA/XYrHg8XinPwOz2SyXyx977DEoo/+Q0bVr14aGhtye3dzcxDoAJBIpLy8vNzf36aefRl/B2djYQBfEy//g+AzW19cJBAK2KxKJNDc359T/hQsXJicn/X5TfX19UqnU7SkCgVBTUxMZGel6qqWl5c6dO48//ji2cHZ2lkqlRkREOO2X29rajvizv3HjRkZGRijKaGNjY25u7u7du/tVwOPx2DTSrVu3FAqFQCB4++23AQC//PJLTExMeno6AGBsbIzFYqHVjEbjzZs3o6OjsV3Z7XY8Hn80TzwhIcFkMu0nI7epP41G09bWRqVSnWTkNjDC4XCPPPLIEcuISCSG6GyEBkbY+NoJCoUyPT2NLREKhRERETdu3AAAjI+PX7lyBZ2BeDxeUVERGrPX1dVNTEz89NNP2Get0+kuXLjguqhdv359b2/P7dVxONxnn33mxaJ29R8O1aS5uXlnZ0cikRwwvr5z5w5c1P4Pn88PDw93zCKu0On0jo6Of+XRw8KIRGJ4eDiCIFNTU8nJyQCA3d1dhULBZDIBAN988w2Xy83MzDSbzdiG8/Pzzz33nOui5rrSHTyU8RBUHQq1Wr29vR0VFSUWi11lxGazfcwY22y2w7of3rU66iw2n8/PzMwkEolnz57Nz8+XyWRuqxmNxujoaNQhQTEYDAUFBRUVFS+++CKXy01NTeVyuSUlJQsLC446n3zyydWrV7EZS5vNFhsbq1Ao/DL4d955h8lkEonEc+fOZWRkVFdX+9jh2NiYRCJhsVgPPfSQU66cRCLpdLqDdLKystLX17ezs4MtNJvNpaWlUqn0sEPSaDQcDsept5BOP3qmpKSko6PD6eGura2p1WrUdl1cXLRYLNgKTCazs7OzsrLSoaQff/yRxWL5a0g2m81Jo37ptry8HF18HSWLi4spKSkHaSsQCDIyMths9lNPPYUt98VHCoSJFKj043/S1NTU09ODdelxOFxCQgL6Xjoej3/44Yed5l46nc7j8XJzc9GUD7qvaWpq8ptBHRaGTSb56z9mSEtLAwDMzMx4kXhsbGx89tlno6OjCwoKHIUCgUAikZSWlno3nuzsbKPRODg4eAys2YPw+eefNzQ0HKqJwWBw/Lurq6uqqir0LQI+nw8AwE4ejY2N77777n82NJlMkZGRg4ODTuW++0iBMJGCMxsBAF555ZVz587Nzs4evMn999/v2EjPzMx8+OGHob9/QWcjbJR9kNnIYDD89ttvZrM5ISEBu6vwi48UKBMJgQQSMpmck5PjOExKSlpZWfHcpLW19dKlSwQCgc1md3Z2OsoHBgYYDIbj0GKxlJWVlZeXczgctOS1116rqalBEKS2tjYxMXG/CK+wsLCtre0kzEanh7S0NEfqSKPR7O7uJiYmem7C5XKffPJJFovV39+PBukobn0kGo32+++/O3yklJQU1EfSaDQOE+mrr77C9u/WRPI1uISfdKBltLm5ubW1BQCYmppiMBgHMYZFIpFT7tutj1RcXDw0NOTZR3JrIqlUKiijYyYjAAA6IR1wm2az2cRisauMXH2kzc1NgUBQXFy8n4+EmkiFhYWBNpGgjEJORjKZbG9vz7UmhULR6XQH95EUCsXLL798+fLlxcVFbCu3JhKUUUhz6dIlx2btgF8qEolEkZGRNBrN1Udy8nmcfCTUanX4SKiJtLS05Goi0el0/94m/PJ1wElKSkpMTBweHj5//rxWq3V6RcSVhoYGgUAgFAqdyk0m08WLF5eWlhzOvNFoLCoqevDBBw0GQ3x8/MDAAJvNXl1dbW5uRj3sTz/9tLe3VygUOgIyu91OoVDEYnFcXBzc8B8nCgoKoqKi/vzzzyeeeMJzTfRd9by8vA8++MAvPlKgTSS44T/S8Eiv1/N4PM8rmkqliouL4/F4IpHo3r17fvGRAm0iwdno6Ojp6QEAJCcnt7e3e6gmk8nIZHJxcbHn3OBhfaSjMZGgjALO+Pg4+hc7OjrquaZer9/vBRvsKtbS0jIzM3PYYfz11191dXVmszkQ9whD7IBjMBjOnj2LflnWj++thhTwN0MCzpkzZ1JTU20220nVEJTR0UXZJ+AXHaCMgkx+fj6ZTD7BNwhjI4gfgHkjCJQRBMoIAmUEgUAZQaCMIFBGECgjCATKCBIg/hcAAP//eHMTX3Uq77wAAAAASUVORK5CYII=)\n",
        "\n",
        "where t is threshold value (heuristically set to 1e-5) and f(w) is frequency of the word.\n",
        "\n",
        "Implement a subsampling mechanism to handle overly frequent words in the corpus. Use the heuristic formula provided by Mikolov et al. to calculate the probability of dropping a word based on its frequency. This step ensures the corpus maintains a balance between rare and frequent words, improving the quality of word co-occurrence relationships.\n",
        "\n",
        "---\n",
        "\n",
        "**Filtering words** - Filter out words that occur only once in the dataset, as they lack sufficient context to be represented effectively. Retain only those words that appear at least five times in the corpus to minimize noise and enhance the overall quality of the embeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-10T13:13:09.551725Z",
          "iopub.status.busy": "2020-10-10T13:13:09.441732Z",
          "iopub.status.idle": "2020-10-10T13:13:11.452524Z",
          "shell.execute_reply": "2020-10-10T13:13:11.451989Z"
        },
        "id": "wp50T2OqA-7L",
        "papermill": {
          "duration": 3.4315,
          "end_time": "2020-10-10T13:13:11.452631",
          "exception": false,
          "start_time": "2020-10-10T13:13:08.021131",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.replace('.', ' . ').replace(',', ' , ').replace(';', ' ; ').replace(':', ' : ')\n",
        "    text = text.lower().strip()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in text.split() if word not in stop_words]\n",
        "    word_counts = Counter(words)\n",
        "    words = [word for word in words if word_counts[word] >= 5]\n",
        "    total_count = sum(word_counts.values())\n",
        "    threshold = 1e-5\n",
        "    word_probabilities = {\n",
        "        word: 1 - math.sqrt(threshold / (count / total_count))\n",
        "        for word, count in word_counts.items()\n",
        "    }\n",
        "    words = [word for word in words if random.random() > word_probabilities.get(word, 0)]\n",
        "    return words, word_counts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4260B2ZFJ_EA",
        "papermill": {
          "duration": 1.777391,
          "end_time": "2020-10-10T13:13:14.603396",
          "exception": false,
          "start_time": "2020-10-10T13:13:12.826005",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "It's always a good idea to take a quick look at preprocessed sample before heading further - you might observe few things that if handled can enrich or correct your data. More like a validation step this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-10T13:13:17.665608Z",
          "iopub.status.busy": "2020-10-10T13:13:17.664211Z",
          "iopub.status.idle": "2020-10-10T13:13:17.667915Z",
          "shell.execute_reply": "2020-10-10T13:13:17.668396Z"
        },
        "id": "_oNvdt-v1dw0",
        "papermill": {
          "duration": 1.689149,
          "end_time": "2020-10-10T13:13:17.668521",
          "exception": false,
          "start_time": "2020-10-10T13:13:15.979372",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e992da52-7953-4c33-e9b3-84accb50fcca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100000000\n",
            "['state', 'anarchist', 'anarchist', 'modern', 'eight', 'anarchists', 'anarchist']\n",
            " anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philosophy is the belief that rulers are unnecessary and should be abolished although there are differing interpretations of what this means anarchism also refers to related social movements that advocate the elimination of authoritarian institutions particularly the state the word anarchy as most anarchists use it does not imply chaos nihilism or anomie but rather a harmonious anti authoritarian society in place of what are regarded as authoritarian political structures and coercive economic institutions anarchists advocate social relations based upon voluntary association of autonomous individuals mutual aid and self governance while anarchism is most easily defined by what it is against anarchists also offer positive visions of what they believe to be a truly free society however ideas about how an anarchist society might work vary considerably especially with respect to economics there is also disagreement about how a free society might be brought about origins and predecessors kropotkin and others argue that before recorded history human society was organized on anarchist principles most anthropologists follow kropotkin and engels in believing that hunter gatherer bands were egalitarian and lacked division of labour accumulated wealth or decreed law and had equal access to resources william godwin anarchists including the the anarchy organisation and rothbard find anarchist attitudes in taoism from ancient china kropotkin found similar ideas in stoic zeno of citium according to kropotkin zeno repudiated the omnipotence of the state its intervention and regimentation and proclaimed the sovereignty of the moral law of the individual the anabaptists of one six th century europe are sometimes considered to be religious forerunners of modern anarchism bertrand russell in his history of western philosophy writes that the anabaptists repudiated all law since they held that the good man will be guided at every moment by the holy spirit from this premise they arrive at communism the diggers or true levellers were an early communistic movement during the time of the english civil war and are considered by some as forerunners of modern anarchism in the modern era the first to use the term to mean something other than chaos was louis armand baron de lahontan in his nouveaux voyages dans l am rique septentrionale one seven zero three where he described the indigenous american society which had no state laws prisons priests or private property as being in anarchy russell means a libertarian and leader in the american indian movement has repeatedly stated that he is an anarchist and so are all his ancestors in one seven nine three in the thick of the french revolution william godwin published an enquiry concerning political justice although godwin did not use the word anarchism many later anarchists have regarded this book as the first major anarchist text and godwin as the founder of philosophical anarchism but at this point no anarchist movement yet existed and the term anarchiste was known mainly as an insult hurled by the bourgeois girondins at more radical elements in the french revolution the first self labelled anarchist pierre joseph proudhon it is commonly held that it wasn t until pierre joseph proudhon published what is property in one eight four zero that the term anarchist was adopted as a self description it is for this reason that some claim proudhon as the founder of modern anarchist theory in what is property proudhon answers with the famous accusation property is theft in this work he opposed the institution of decreed property propri t where owners have complete rights to use and abuse their property as they wish such as exploiting workers for profit in its place proudhon supported what he called possession individuals can have limited rights to use resources capital and goods in accordance with principles of equality and justice proudhon s vision of anarchy which he called mutualism mutuellisme involved an exchange economy where individuals and groups could trade the products of their labor using labor notes which represented the amount of working time involved in production this would ensure that no one would profit from the labor of others workers could freely join together in co operative workshops an interest free bank would be set up to provide everyone with access to the means of production proudhon s ideas were influential within french working class movements and his followers were active in the revolution of one eight four eight in france proudhon s philosophy of property is complex it was developed in a number of works over his lifetime and there are differing interpretations of some of his ideas for more detailed discussion see here max stirner s egoism in his the ego and its own stirner argued that most commonly accepted social institutions including the notion of state property as a right natural rights in general and the very notion of society were mere illusions or ghosts in the mind saying of society that the individuals are its reality he advocated egoism and a form of amoralism in which individuals would unite in associations of egoists only when it was in their self interest to do so for him property simply comes about through might whoever knows how to take to defend the thing to him belongs property and what i have in my power that is my own so long as i assert myself as holder i am the proprietor of the thing stirner never called himself an anarchist he accepted only the label egoist nevertheless his ideas were influential on many individualistically inclined anarchists although interpretations of his thought are diverse american individualist anarchism benjamin tucker in one eight two five josiah warren had participated in a communitarian experiment headed by robert owen called new harmony which failed in a few years amidst much internal conflict warren blamed the community s failure on a lack of individual sovereignty and a lack of private property warren proceeded to organise experimenal anarchist communities which respected what he called the sovereignty of the individual at utopia and modern times in one eight three three warren wrote and published the peaceful revolutionist which some have noted to be the first anarchist periodical ever published benjamin tucker says that warren was the first man to expound and formulate the doctrine now known as anarchism liberty xiv december one nine zero zero one benjamin tucker became interested in anarchism through meeting josiah warren and william b greene he edited and published liberty from august one eight eight one to april one nine zero eight it is widely considered to be the finest individualist anarchist periodical ever issued in the english language tucker s conception of individualist anarchism incorporated the ideas of a variety of theorists greene s ideas on mutual banking warren s ideas on cost as the limit of price a heterodox variety of labour theory of value proudhon s market anarchism max stirner s egoism and herbert spencer s law of equal freedom tucker strongly supported the individual s right to own the product of his or her labour as private property and believed in a market economy for trading this property he argued that in a truly free market system without the state the abundance of competition would eliminate profits and ensure that all workers received the full value of their labor other one nine th century individualists included lysander spooner stephen pearl andrews and victor yarros the first international mikhail bakunin one eight one four one eight seven six in europe harsh reaction followed the revolutions of one eight four eight twenty years later in one eight six four the international workingmen s association sometimes called the first international united some diverse european revolutionary currents including anarchism due to its genuine links to active workers movements the international became signficiant from the start karl marx was a leading figure in the international he was elected to every succeeding general council of the association the first objections to marx came from the mutualists who opposed communism and statism shortly after mikhail bakunin and his followers joined in one eight six eight the first international became polarised into two camps with marx and bakunin as their respective figureheads the clearest difference between the camps was over strategy the anarchists around bakunin favoured in kropotkin s words direct economical struggle against capitalism without interfering in the political parliamentary agitation at that time marx and his followers focused on parliamentary activity bakunin characterised marx s ideas as authoritarian and predicted that if a marxist party gained to power its leaders would end up as bad as the ruling class they had fought against in one eight seven two the conflict climaxed with a final split between the two groups at the hague congress this is often cited as the origin of the conflict between anarchists and marxists from this moment the social democratic and libertarian currents of socialism had distinct organisations including rival internationals anarchist communism peter kropotkin proudhon and bakunin both opposed communism associating it with statism however in the one eight seven zero s many anarchists moved away from bakunin s economic thinking called collectivism and embraced communist concepts communists believed the means of production should be owned\n",
            "[('anarchism', 15), ('originated', 1), ('term', 5), ('abuse', 2), ('first', 10), ('used', 3), ('early', 2), ('working', 3), ('class', 3), ('radicals', 1)]\n"
          ]
        }
      ],
      "source": [
        "sample = wiki[:10000]\n",
        "print(len(wiki))\n",
        "processed_texts,count_words = preprocess_text(sample)\n",
        "print(processed_texts)\n",
        "print(sample)\n",
        "print(list(count_words.items())[:10])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCkFtaa_KrTb",
        "papermill": {
          "duration": 1.426874,
          "end_time": "2020-10-10T13:13:20.673211",
          "exception": false,
          "start_time": "2020-10-10T13:13:19.246337",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### Hyperparameters\n",
        "Setting a few hyperparamters required for gnerating batches and for deciding the size of word embeddings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-10T13:13:23.483005Z",
          "iopub.status.busy": "2020-10-10T13:13:23.482371Z",
          "iopub.status.idle": "2020-10-10T13:13:23.486821Z",
          "shell.execute_reply": "2020-10-10T13:13:23.486338Z"
        },
        "id": "mJLzBkSIKoMx",
        "papermill": {
          "duration": 1.447402,
          "end_time": "2020-10-10T13:13:23.486929",
          "exception": false,
          "start_time": "2020-10-10T13:13:22.039527",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "EMBEDDING_DIM = 128\n",
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 512\n",
        "EPOCHS = 5\n",
        "WINDOW_SIZE = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oO7N0ZsLofI",
        "papermill": {
          "duration": 1.481198,
          "end_time": "2020-10-10T13:13:26.663213",
          "exception": false,
          "start_time": "2020-10-10T13:13:25.182015",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### Preparing TensorFlow Dataset using Skipgrams\n",
        "\n",
        "**Generating Skipgrams**\n",
        "\n",
        "Tokenize your preprocessed textual data and convert the words into their corresponding vectorized tokens. Then, use the `skipgrams` function provided by Keras to generate (word, context) pairs. Ensure the following steps are completed:\n",
        "\n",
        "- Generate positive samples: (word, word in the same window), with label 1.  \n",
        "- Generate negative samples: (word, random word from the vocabulary), with label 0.  \n",
        "\n",
        "Refer to Mikolov et al.'s paper, [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781v3.pdf), for more details on Skipgrams.\n",
        "\n",
        "---\n",
        "\n",
        "**Negative Sampling**\n",
        "\n",
        "For each input word, implement the negative sampling approach to optimize the training process. Transform the problem of predicting context words into independent binary classification tasks. For every (target, context) pair, generate random negative (target, ~context) samples. This step will reduce computational complexity and make training more efficient.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-10T13:13:29.493673Z",
          "iopub.status.busy": "2020-10-10T13:13:29.483476Z",
          "iopub.status.idle": "2020-10-10T13:14:02.707005Z",
          "shell.execute_reply": "2020-10-10T13:14:02.705890Z"
        },
        "id": "Uq4-jfYOjonO",
        "papermill": {
          "duration": 34.646886,
          "end_time": "2020-10-10T13:14:02.707124",
          "exception": false,
          "start_time": "2020-10-10T13:13:28.060238",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88c794a9-c0da-4ac3-e6f4-ac4852a7aeba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000000\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
        "index = int(len(wiki)/10)\n",
        "print(index)\n",
        "processed_words,_ = preprocess_text(wiki[:index])\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(processed_words)\n",
        "vocab_mapping = tokenizer.word_index\n",
        "vocab_count = len(vocab_mapping) + 1\n",
        "\n",
        "token_sequences = tokenizer.texts_to_sequences([processed_words])[0]\n",
        "\n",
        "pair_data, pair_labels = skipgrams(\n",
        "    token_sequences,\n",
        "    vocabulary_size=vocab_count,\n",
        "    window_size=5,\n",
        "    negative_samples=5,\n",
        ")\n",
        "\n",
        "word_targets, word_contexts = zip(*pair_data)\n",
        "word_targets = np.array(word_targets, dtype=np.int32)\n",
        "word_contexts = np.array(word_contexts, dtype=np.int32)\n",
        "pair_labels = np.array(pair_labels, dtype=np.int32)\n",
        "\n",
        "dataset_size = len(word_targets)\n",
        "training_set_size = int(dataset_size * 0.8)\n",
        "all_indices = np.arange(dataset_size)\n",
        "np.random.shuffle(all_indices)\n",
        "\n",
        "train_idx = all_indices[:training_set_size]\n",
        "test_idx = all_indices[training_set_size:]\n",
        "\n",
        "train_word_targets = word_targets[train_idx]\n",
        "train_word_contexts = word_contexts[train_idx]\n",
        "train_pair_labels = pair_labels[train_idx]\n",
        "\n",
        "test_word_targets = word_targets[test_idx]\n",
        "test_word_contexts = word_contexts[test_idx]\n",
        "test_pair_labels = pair_labels[test_idx]\n",
        "\n",
        "train_tf_dataset = tf.data.Dataset.from_tensor_slices(((train_word_targets, train_word_contexts), train_pair_labels))\n",
        "train_tf_dataset = train_tf_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_tf_dataset = tf.data.Dataset.from_tensor_slices(((test_word_targets, test_word_contexts), test_pair_labels))\n",
        "test_tf_dataset = test_tf_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-10T13:14:05.592072Z",
          "iopub.status.busy": "2020-10-10T13:14:05.591186Z",
          "iopub.status.idle": "2020-10-10T13:14:12.984172Z",
          "shell.execute_reply": "2020-10-10T13:14:12.984873Z"
        },
        "id": "JRHxw7X4zOpg",
        "papermill": {
          "duration": 8.912486,
          "end_time": "2020-10-10T13:14:12.985042",
          "exception": false,
          "start_time": "2020-10-10T13:14:04.072556",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90d04975-68f9-407e-fa37-4c49e06e739c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "size of training dataset: 29372\n",
            "size of test dataset: 7343\n"
          ]
        }
      ],
      "source": [
        "print(f\"size of training dataset: {len(list(train_tf_dataset))}\")\n",
        "print(f\"size of test dataset: {len(list(test_tf_dataset))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUatOx50OXF1",
        "papermill": {
          "duration": 1.350656,
          "end_time": "2020-10-10T13:14:15.692486",
          "exception": false,
          "start_time": "2020-10-10T13:14:14.341830",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### Building the Model\n",
        "\n",
        "Use the model subclassing method to build your model. While Sequential and Functional APIs are generally more suitable for most use cases, model subclassing allows you to create the model in an object-oriented way. Follow these steps:\n",
        "\n",
        "1. Define a custom model class by inheriting from `tf.keras.Model`.\n",
        "2. Implement the `__init__` method to define the layers of your model.\n",
        "3. Override the `call` method to define the forward pass of your model.\n",
        "4. Ensure that the model includes embedding layers, a skip-gram architecture, and any other necessary components for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-10T13:14:18.975942Z",
          "iopub.status.busy": "2020-10-10T13:14:18.969008Z",
          "iopub.status.idle": "2020-10-10T13:14:19.276913Z",
          "shell.execute_reply": "2020-10-10T13:14:19.276347Z"
        },
        "id": "6gLxFZ9Eu9Tw",
        "papermill": {
          "duration": 1.935377,
          "end_time": "2020-10-10T13:14:19.277030",
          "exception": false,
          "start_time": "2020-10-10T13:14:17.341653",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "311dacb2-c7b6-4047-b1b2-9cc6e53376d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[0.50075924]\n",
            " [0.50163156]\n",
            " [0.50026256]], shape=(3, 1), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "class SkipGramModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(SkipGramModel, self).__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
        "        self.target_dense = tf.keras.layers.Dense(units=1, activation=\"sigmoid\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        target, context = inputs\n",
        "        target_embedding = self.embedding(target)\n",
        "        context_embedding = self.embedding(context)\n",
        "        dot_product = tf.reduce_sum(target_embedding * context_embedding, axis=-1)\n",
        "        dot_product = tf.expand_dims(dot_product, axis=-1)  # Add an additional dimension\n",
        "        output = self.target_dense(dot_product)\n",
        "        return output\n",
        "\n",
        "embedding_dim = EMBEDDING_DIM\n",
        "model = SkipGramModel(vocab_size=vocab_count, embedding_dim=embedding_dim)\n",
        "\n",
        "sample_target = tf.constant([1, 2, 3], dtype=tf.int32)\n",
        "sample_context = tf.constant([4, 5, 6], dtype=tf.int32)\n",
        "model_output = model((sample_target, sample_context))\n",
        "print(model_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RN3SV3zv0pXG",
        "papermill": {
          "duration": 1.56129,
          "end_time": "2020-10-10T13:14:22.236946",
          "exception": false,
          "start_time": "2020-10-10T13:14:20.675656",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### Loss function, Metrics and Optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-10T13:14:25.048870Z",
          "iopub.status.busy": "2020-10-10T13:14:25.048241Z",
          "iopub.status.idle": "2020-10-10T13:14:25.068088Z",
          "shell.execute_reply": "2020-10-10T13:14:25.067312Z"
        },
        "id": "ENLrMWOtpixA",
        "papermill": {
          "duration": 1.420264,
          "end_time": "2020-10-10T13:14:25.068193",
          "exception": false,
          "start_time": "2020-10-10T13:14:23.647929",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
        "train_acc_metric = tf.keras.metrics.BinaryAccuracy()\n",
        "val_acc_metric = tf.keras.metrics.BinaryAccuracy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eyQ_o1EWuJA",
        "papermill": {
          "duration": 1.384257,
          "end_time": "2020-10-10T13:14:27.862984",
          "exception": false,
          "start_time": "2020-10-10T13:14:26.478727",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### Training the Model\n",
        "\n",
        "Implement custom training for learning word embeddings to gain finer control over optimization and training tasks. Follow these steps:\n",
        "\n",
        "1. Define a custom training loop that includes forward propagation, loss computation, and backpropagation.\n",
        "2. Use the optimizer of your choice to update the model's weights based on the computed gradients.\n",
        "3. Implement batching for efficient data processing during training.\n",
        "4. Monitor the loss during each epoch to track the model's performance.\n",
        "5. Save the trained embeddings for later use once the training is complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-10T13:14:30.800520Z",
          "iopub.status.busy": "2020-10-10T13:14:30.799563Z",
          "iopub.status.idle": "2020-10-10T13:36:46.204754Z",
          "shell.execute_reply": "2020-10-10T13:36:46.205408Z"
        },
        "id": "oHNb85OL29hu",
        "papermill": {
          "duration": 1336.991023,
          "end_time": "2020-10-10T13:36:46.205587",
          "exception": false,
          "start_time": "2020-10-10T13:14:29.214564",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "682cc25c-6697-4277-b932-8a22ec72f650"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Training loss (batch 0): 0.4321709871292114\n",
            "Training loss (batch 5000): 0.359577476978302\n",
            "Training loss (batch 10000): 0.3518892526626587\n",
            "Training loss (batch 15000): 0.3748735189437866\n",
            "Training loss (batch 20000): 0.2833389341831207\n",
            "Training loss (batch 25000): 0.30349141359329224\n",
            "Training accuracy: 0.8620156049728394\n",
            "Validation accuracy: 0.8791515827178955\n",
            "-------------------------------------------\n",
            "Epoch 2/5\n",
            "Training loss (batch 0): 0.2925770878791809\n",
            "Training loss (batch 5000): 0.26350969076156616\n",
            "Training loss (batch 10000): 0.2213103473186493\n",
            "Training loss (batch 15000): 0.27283167839050293\n",
            "Training loss (batch 20000): 0.26016175746917725\n",
            "Training loss (batch 25000): 0.26193559169769287\n",
            "Training accuracy: 0.8999175429344177\n",
            "Validation accuracy: 0.8859329223632812\n",
            "-------------------------------------------\n",
            "Epoch 3/5\n",
            "Training loss (batch 0): 0.21735258400440216\n",
            "Training loss (batch 5000): 0.26437222957611084\n",
            "Training loss (batch 10000): 0.21560600399971008\n",
            "Training loss (batch 15000): 0.17366112768650055\n",
            "Training loss (batch 20000): 0.1955275982618332\n",
            "Training loss (batch 25000): 0.190626859664917\n",
            "Training accuracy: 0.9142459630966187\n",
            "Validation accuracy: 0.8879600763320923\n",
            "-------------------------------------------\n",
            "Epoch 4/5\n",
            "Training loss (batch 0): 0.1845565140247345\n",
            "Training loss (batch 5000): 0.22137463092803955\n",
            "Training loss (batch 10000): 0.15384361147880554\n",
            "Training loss (batch 15000): 0.1846388727426529\n",
            "Training loss (batch 20000): 0.175430566072464\n",
            "Training loss (batch 25000): 0.17180730402469635\n",
            "Training accuracy: 0.9228610992431641\n",
            "Validation accuracy: 0.8886979222297668\n",
            "-------------------------------------------\n",
            "Epoch 5/5\n",
            "Training loss (batch 0): 0.14259576797485352\n",
            "Training loss (batch 5000): 0.17228209972381592\n",
            "Training loss (batch 10000): 0.1867845058441162\n",
            "Training loss (batch 15000): 0.14941024780273438\n",
            "Training loss (batch 20000): 0.1872636377811432\n",
            "Training loss (batch 25000): 0.1709262728691101\n",
            "Training accuracy: 0.928875207901001\n",
            "Validation accuracy: 0.8890822529792786\n",
            "-------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "@tf.function\n",
        "def train_step(inputs, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(inputs)\n",
        "        loss = loss_fn(labels, predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "    train_acc_metric.update_state(labels, predictions)\n",
        "    return loss\n",
        "\n",
        "@tf.function\n",
        "def test_step(inputs, labels):\n",
        "    predictions = model(inputs)\n",
        "    loss = loss_fn(labels, predictions)\n",
        "    val_acc_metric.update_state(labels, predictions)\n",
        "    return loss\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
        "    for batch, ((target, context), labels) in enumerate(train_tf_dataset):\n",
        "        loss = train_step((target, context), labels)\n",
        "        if batch % 5000 == 0:\n",
        "            print(f\"Training loss (batch {batch}): {loss.numpy()}\")\n",
        "\n",
        "    train_accuracy = train_acc_metric.result()\n",
        "    print(f\"Training accuracy: {train_accuracy.numpy()}\")\n",
        "    train_acc_metric.reset_state()\n",
        "\n",
        "    for ((target, context), labels) in test_tf_dataset:\n",
        "        val_loss = test_step((target, context), labels)\n",
        "\n",
        "    val_accuracy = val_acc_metric.result()\n",
        "    print(f\"Validation accuracy: {val_accuracy.numpy()}\")\n",
        "    val_acc_metric.reset_state()\n",
        "\n",
        "    print(\"-------------------------------------------\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-10T13:36:48.935958Z",
          "iopub.status.busy": "2020-10-10T13:36:48.935357Z",
          "iopub.status.idle": "2020-10-10T13:36:48.973240Z",
          "shell.execute_reply": "2020-10-10T13:36:48.972739Z"
        },
        "id": "V2iWMNGQahsc",
        "papermill": {
          "duration": 1.409531,
          "end_time": "2020-10-10T13:36:48.973383",
          "exception": false,
          "start_time": "2020-10-10T13:36:47.563852",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "634670e6-cd84-483a-e7cc-3faa75104f7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights saved to checkpoint at ./chckpt/model\n"
          ]
        }
      ],
      "source": [
        "checkpoint_prefix = \"./chckpt/model\"\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\n",
        "\n",
        "checkpoint.save(file_prefix=checkpoint_prefix)\n",
        "print(f\"Weights saved to checkpoint at {checkpoint_prefix}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb7Wx_rzZOOm",
        "papermill": {
          "duration": 1.591393,
          "end_time": "2020-10-10T13:36:51.938622",
          "exception": false,
          "start_time": "2020-10-10T13:36:50.347229",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### Word Embeddings Projector\n",
        "\n",
        "Follow these steps to visualize the learned word embeddings using TensorFlow's Embedding Projector:\n",
        "\n",
        "1. Extract the weights of the embedding layer from your trained model.\n",
        "2. Save the weights into two files:\n",
        "   - `vecs.tsv`: This file will store the actual vector representations of words.\n",
        "   - `meta.tsv`: This file will store the associated metadata (e.g., word labels) for visualization.\n",
        "3. Go to [TensorFlow Embedding Projector](http://projector.tensorflow.org/).\n",
        "4. Upload the `vecs.tsv` and `meta.tsv` files created in the previous step.\n",
        "5. Explore the visualizations provided by TensorFlow's Embedding Projector.\n",
        "\n",
        "</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-10T13:36:54.675412Z",
          "iopub.status.busy": "2020-10-10T13:36:54.674309Z",
          "iopub.status.idle": "2020-10-10T13:36:56.018539Z",
          "shell.execute_reply": "2020-10-10T13:36:56.017942Z"
        },
        "id": "fGpXtNRS-V_u",
        "papermill": {
          "duration": 2.703171,
          "end_time": "2020-10-10T13:36:56.018656",
          "exception": false,
          "start_time": "2020-10-10T13:36:53.315485",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c412d5e8-ec27-46eb-f4fa-cab220fa7ffb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings and metadata have been saved.\n"
          ]
        }
      ],
      "source": [
        "embedding_layer = model.embedding\n",
        "\n",
        "embedding_weights = embedding_layer.get_weights()[0]\n",
        "\n",
        "with open('vecs.tsv', 'w') as vecs_file, open('meta.tsv', 'w') as meta_file:\n",
        "\n",
        "    for word, idx in tokenizer.word_index.items():\n",
        "        embedding = embedding_weights[idx]\n",
        "        vecs_file.write('\\t'.join(map(str, embedding)) + '\\n')\n",
        "        meta_file.write(word + '\\n')\n",
        "\n",
        "\n",
        "print(\"Embeddings and metadata have been saved.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "papermill": {
      "duration": 1467.163823,
      "end_time": "2020-10-10T13:37:04.319726",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2020-10-10T13:12:37.155903",
      "version": "2.1.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}